{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Practical Deep Learning Tutorial with PyTorch - Tutorial N° 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2020-2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import grad\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Built ADALINE model using the nn.Module class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Model,self).__init__()\n",
    "            self.layer = nn.Linear(4,1)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        def forward(self, x):\n",
    "            x=self.layer(torch.tensor(x,dtype=torch.float))\n",
    "            return x\n",
    "           \n",
    "        def predict(self,x,threshold=0.5):\n",
    "            with torch.no_grad():\n",
    "                y_pred=self.forward(x)\n",
    "                y_pred=(y_pred >= threshold).float()\n",
    "            return y_pred   \n",
    "        \n",
    "model=Model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using 'iris.txt', create a binary datasets in 2-D : The last 100 instances of iris described only by the 2nd and 3rd features\n",
    "    \n",
    "    Split the dataset into traing and test sets (70%,30%) \n",
    "\n",
    "    Normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=pd.read_csv('/home/ing/Bureau/Deep_learning/iris.txt',header=None)\n",
    "y=iris[iris.columns[4]]\n",
    "y=y[50:]\n",
    "y=y.factorize()[0]\n",
    "iris.drop(columns=4,axis=0,inplace=True)\n",
    "# the last hundred binary classes\n",
    "\n",
    "\n",
    "\n",
    "iris=iris[50:]\n",
    "# scaling and spliting the dataset \n",
    "scaler=StandardScaler()\n",
    "iris=scaler.fit_transform(iris)\n",
    "x_train,x_test,y_train,y_test=train_test_split(iris,y,test_size=0.3)\n",
    "x_train=torch.as_tensor(x_train)\n",
    "y_train=torch.as_tensor(y_train)\n",
    "\n",
    "x_test=torch.as_tensor(x_test)\n",
    "y_test=torch.as_tensor(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Train the model : we will use MSELoss (mean squared error (squared L2 norm)) as loss function. The optimizer is SGD (Stochastic Gradient Descent) with learning rate 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3734/3916511740.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs=torch.tensor(x_train,dtype=torch.float)\n",
      "/tmp/ipykernel_3734/248798347.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x=self.layer(torch.tensor(x,dtype=torch.float))\n",
      "/tmp/ipykernel_3734/3916511740.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss=criterion(outputs,torch.tensor(y_train,dtype=torch.float).reshape(-1,1))\n"
     ]
    }
   ],
   "source": [
    "criterion=nn.MSELoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.01)\n",
    "inputs=torch.tensor(x_train,dtype=torch.float)\n",
    "for epoch in range(1000):\n",
    "# calculer les prédictions  \n",
    "    outputs=model.forward(inputs)\n",
    "    # calucler la perte\n",
    "    loss=criterion(outputs,torch.tensor(y_train,dtype=torch.float).reshape(-1,1))\n",
    "    # Réinitialiser les gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Propager la perte arrière\n",
    "    loss.backward()\n",
    "    # Mettre à jour les poids\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compute the model accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3734/248798347.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x=self.layer(torch.tensor(x,dtype=torch.float))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Built a Perceptron model using nn.Module class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class perceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(perceptron,self).__init__()\n",
    "        self.layer=nn.Linear(2,1,dtype=float)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "            out=self.layer(x)\n",
    "            out=self.sigmoid(out)\n",
    "            return out\n",
    "\n",
    "\n",
    "    def predict(self,x,threshold=0.5):\n",
    "            with torch.no_grad():\n",
    "                y_pred=self.forward(x)\n",
    "                y_pred=(y_pred >= threshold)\n",
    "            return y_pred\n",
    "        \n",
    "percep=perceptron()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Load the 'perceptron_toydata' dataset\n",
    "\n",
    "    Split the dataset into train and test sets\n",
    "    \n",
    "    Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "percep_toydata=pd.read_csv('/home/ing/Bureau/Deep_learning/perceptron_toydata.txt',sep='\\t',header=None)\n",
    "\n",
    "y=percep_toydata[percep_toydata.columns[2]]\n",
    "\n",
    "percep_toydata.drop(columns=2,inplace=True)\n",
    "\n",
    "x=percep_toydata.to_numpy()\n",
    "\n",
    "x=scaler.fit_transform(x)\n",
    "x_train1,x_test1,y_train1,y_test1=train_test_split(x,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Train the perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 70])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion\n",
    "y_train.shape\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.BCELoss()\n",
    "optimizer=torch.optim.SGD(percep.parameters(),lr=0.01)\n",
    "inputs=torch.tensor(x_train,dtype=float)\n",
    "for epoch in range(1000):\n",
    "# calculer les prédictions  \n",
    "    outputs=percep.forward(inputs)\n",
    "    # calucler la perte\n",
    "    \n",
    "    loss=criterion(outputs,torch.tensor(y_train,dtype=float).reshape((-1,1)))\n",
    "    # Réinitialiser les gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Propager la perte arrière\n",
    "    loss.backward()\n",
    "    # Mettre à jour les poids\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. evaluate the model (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3734/2923844667.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test1=torch.tensor(x_test1)\n",
      "/tmp/ipykernel_3734/2923844667.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_train1=torch.tensor(x_train1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test1=torch.tensor(x_test1)\n",
    "x_train1=torch.tensor(x_train1)\n",
    "def accuracy(model,x,y):\n",
    "    y_pred=model.predict(x)\n",
    "    return len(y_pred==y)/len(y)\n",
    "\n",
    "accuracy(percep,x_test1,y_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the single-layer perceptron, the Multi Layer Perceptron models have hidden layers\n",
    "between the input and the output layers. After every hidden layer, an activation function \n",
    "is applied to introduce non-linearity. \n",
    "\n",
    "9. Built a simple Multi Layer Perceptron model withe one hidden layer. \n",
    "After the hidden layer, we will use ReLU as activation before the information is sent to the output layer.\n",
    "As an output activation function, we will use Sigmoid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,input_size,hiden_size,output_size):\n",
    "        super(MLP,self).__init__()\n",
    "        self.layer1=nn.Linear(input_size,hiden_size)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.layer2=nn.Linear(hiden_size,output_size)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        hiden=self.layer1(x)\n",
    "        hiden=self.relu(hiden)\n",
    "        out=self.layer2(hiden)\n",
    "        out=self.softmax(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def predict(self,x,threshold=0.5):\n",
    "        with torch.no_grad():\n",
    "            out=self.forward(x)\n",
    "            y_pred=torch.argmax(out)\n",
    "            return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create a random datasets and assign binary labels {0,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_binary_data(num_samples,input_dim):\n",
    "    X=torch.randn(num_samples,input_dim)\n",
    "    y=torch.randint(0,2,(num_samples,))\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 2])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples=1000\n",
    "input_dim=2\n",
    "binary_dataset=generate_binary_data(num_samples,input_dim)[0]\n",
    "label=generate_binary_data(num_samples,input_dim)[1]\n",
    "binary_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Define the model with input dimension 2 and hidden dimension 10. \n",
    "Since the task is to classify binary labels, we can use BCELoss (Binary Cross Entropy Loss) as loss function.\n",
    "The optimizer is SGD (Stochastic Gradient Descent) with learning rate 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp=MLP(input_size=2,hiden_size=10,output_size=2)\n",
    "\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(binary_dataset,label,train_size=0.7)\n",
    "train_data = TensorDataset(xtrain,ytrain)\n",
    "batch_size=10\n",
    "trainloader=DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3734/1381243240.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss_=criterion_(torch.tensor(output,dtype=float,requires_grad=True),torch.tensor(labels,dtype=float).reshape((-1,1)))\n",
      "/home/ing/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss: 0.261\n",
      "epoch 1000 loss: 0.261\n"
     ]
    }
   ],
   "source": [
    "criterion_=nn.MSELoss()\n",
    "optimizer_=torch.optim.SGD(params=mlp.parameters(),lr=0.01)\n",
    "for epoch in range(1000):\n",
    "    running_loss=0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        input,labels=data\n",
    "         #initialiser le gradient\n",
    "        optimizer_.zero_grad()\n",
    "        # calculer les sorties\n",
    "        output=mlp.forward(input)\n",
    "        \n",
    "            #calculer la perte\n",
    "        loss_=criterion_(torch.tensor(output,dtype=float,requires_grad=True),torch.tensor(labels,dtype=float).reshape((-1,1)))\n",
    "        \n",
    "        # retropropagation du gradient\n",
    "        loss_.backward()\n",
    "        # mettre à jour les paramettres\n",
    "        optimizer_.step()\n",
    "        \n",
    "\n",
    "        running_loss += loss_.item()\n",
    "    if epoch== 0 or epoch== 999:\n",
    "        print('epoch %d loss: %.3f' %(epoch + 1,running_loss /len(trainloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Check the test loss before the model training and compare it with the test loss after the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(mlp,xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. In order to improve the model, you can try out different parameter values for your\n",
    "hyperparameters(ie. hidden dimension size, epoch size, learning rates). You can also \n",
    "try changing the structure of your model (ie. adding more hidden layers) to see if your\n",
    "mode improves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
